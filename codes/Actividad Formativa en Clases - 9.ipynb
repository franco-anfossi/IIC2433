{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IIC-2433 Minería de Datos UC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Versiones de librerías, python 3.8.10\n",
    "\n",
    "- numpy 1.20.3\n",
    "- sklearn 1.0.2\n",
    "- nltk 3.7\n",
    "- keras 2.9.0\n",
    "- tensorflow 2.9.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Actividad en clase\n",
    "\n",
    "Usando el algoritmo **VAE**, haga lo siguiente:\n",
    "\n",
    "- Entrene el **VAE** sobre el dataset de mnist digits a 28x28 (el del jupyter 13). Use la partición de entrenamiento.\n",
    "- Entrene una **nuSVC** a 0.1 sobre los primeros 5000 ejemplos de entrenamiento del dataset. Fíjese que la nuSVC es unidimensional, pero sus datos son 2D, por lo que va a tener que hacer un reshape.\n",
    "- Evalúe el clasificador sobre los primeros 1000 ejemplos de testing.\n",
    "- Genere un dígito con el VAE.\n",
    "- Revise la escala del ejemplo generado con el VAE y hagalo consistente con la escala de los ejemplos usados para entrenar el clasificador. Va a tener que reescalar usanto la transformación min max. Hágala Ud. mismo.\n",
    "- Usando el dígito sintético generado por el VAE y reescalado, páselo por el clasificador. ¿El resultado es correcto?\n",
    "- Cuanto termine, me avisa para entregarle una **L (logrado)**.\n",
    "- Recuerde que cada L es una décima más en la nota de la asignatura.\n",
    "- Pueden trabajar de a dos.\n",
    "\n",
    "***Tiene hasta el final de la clase.***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solución"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-25 23:15:40.681296: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling(layers.Layer):\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 28, 28, 1)]  0           []                               \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 14, 14, 32)   320         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 7, 7, 16)     4624        ['conv2d[0][0]']                 \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 784)          0           ['conv2d_1[0][0]']               \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 16)           12560       ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " z_mean (Dense)                 (None, 2)            34          ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " z_log_var (Dense)              (None, 2)            34          ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " sampling (Sampling)            (None, 2)            0           ['z_mean[0][0]',                 \n",
      "                                                                  'z_log_var[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 17,572\n",
      "Trainable params: 17,572\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-25 23:15:43.541552: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-12.0/lib64:/usr/local/cuda-11.8/lib64\n",
      "2024-05-25 23:15:43.576534: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2024-05-25 23:15:43.577118: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "latent_dim = 2\n",
    "\n",
    "encoder_inputs = keras.Input(shape=(28, 28, 1))\n",
    "x = layers.Conv2D(32, 3, activation=\"relu\", strides=2, padding=\"same\")(encoder_inputs) \n",
    "x = layers.Conv2D(16, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(16, activation=\"relu\")(x)\n",
    "z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
    "z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
    "z = Sampling()([z_mean, z_log_var])\n",
    "encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 2)]               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 784)               2352      \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 7, 7, 16)          0         \n",
      "                                                                 \n",
      " conv2d_transpose (Conv2DTra  (None, 14, 14, 32)       4640      \n",
      " nspose)                                                         \n",
      "                                                                 \n",
      " conv2d_transpose_1 (Conv2DT  (None, 28, 28, 64)       18496     \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " conv2d_transpose_2 (Conv2DT  (None, 28, 28, 1)        577       \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 26,065\n",
      "Trainable params: 26,065\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "latent_inputs = keras.Input(shape=(latent_dim,))\n",
    "x = layers.Dense(7 * 7 * 16, activation=\"relu\")(latent_inputs)\n",
    "x = layers.Reshape((7, 7, 16))(x)\n",
    "x = layers.Conv2DTranspose(32, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "x = layers.Conv2DTranspose(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "decoder_outputs = layers.Conv2DTranspose(1, 3, activation=\"sigmoid\", padding=\"same\")(x)\n",
    "decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(keras.Model):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
    "            name=\"reconstruction_loss\"\n",
    "        )\n",
    "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "        ]\n",
    "\n",
    "    def train_step(self, data):\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var, z = self.encoder(data)\n",
    "            reconstruction = self.decoder(z)\n",
    "            reconstruction_loss = tf.reduce_mean(\n",
    "                tf.reduce_sum(\n",
    "                    keras.losses.binary_crossentropy(data, reconstruction), axis=(1, 2)\n",
    "                )\n",
    "            )\n",
    "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
    "            total_loss = reconstruction_loss + kl_loss\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, _), (x_test, _) = keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "547/547 [==============================] - 17s 30ms/step - loss: 272.3409 - reconstruction_loss: 210.5092 - kl_loss: 2.7102\n",
      "Epoch 2/30\n",
      "547/547 [==============================] - 16s 29ms/step - loss: 177.1413 - reconstruction_loss: 169.9348 - kl_loss: 4.3813\n",
      "Epoch 3/30\n",
      "547/547 [==============================] - 16s 30ms/step - loss: 168.3144 - reconstruction_loss: 161.9209 - kl_loss: 4.8341\n",
      "Epoch 4/30\n",
      "547/547 [==============================] - 17s 31ms/step - loss: 163.5134 - reconstruction_loss: 157.4353 - kl_loss: 5.1477\n",
      "Epoch 5/30\n",
      "547/547 [==============================] - 17s 31ms/step - loss: 160.7042 - reconstruction_loss: 154.9619 - kl_loss: 5.3287\n",
      "Epoch 6/30\n",
      "547/547 [==============================] - 17s 31ms/step - loss: 159.2164 - reconstruction_loss: 153.5233 - kl_loss: 5.4293\n",
      "Epoch 7/30\n",
      "547/547 [==============================] - 17s 31ms/step - loss: 158.0317 - reconstruction_loss: 152.3881 - kl_loss: 5.5051\n",
      "Epoch 8/30\n",
      "547/547 [==============================] - 17s 31ms/step - loss: 157.1530 - reconstruction_loss: 151.5609 - kl_loss: 5.5660\n",
      "Epoch 9/30\n",
      "547/547 [==============================] - 17s 32ms/step - loss: 156.2403 - reconstruction_loss: 150.8133 - kl_loss: 5.6317\n",
      "Epoch 10/30\n",
      "547/547 [==============================] - 17s 31ms/step - loss: 156.0535 - reconstruction_loss: 150.1778 - kl_loss: 5.6768\n",
      "Epoch 11/30\n",
      "547/547 [==============================] - 17s 31ms/step - loss: 155.2106 - reconstruction_loss: 149.5404 - kl_loss: 5.7242\n",
      "Epoch 12/30\n",
      "547/547 [==============================] - 17s 32ms/step - loss: 155.1599 - reconstruction_loss: 149.1558 - kl_loss: 5.7460\n",
      "Epoch 13/30\n",
      "547/547 [==============================] - 18s 32ms/step - loss: 154.7310 - reconstruction_loss: 148.6589 - kl_loss: 5.7863\n",
      "Epoch 14/30\n",
      "547/547 [==============================] - 18s 32ms/step - loss: 154.1566 - reconstruction_loss: 148.3471 - kl_loss: 5.8152\n",
      "Epoch 15/30\n",
      "547/547 [==============================] - 17s 32ms/step - loss: 153.8179 - reconstruction_loss: 148.0360 - kl_loss: 5.8322\n",
      "Epoch 16/30\n",
      "547/547 [==============================] - 18s 32ms/step - loss: 153.5043 - reconstruction_loss: 147.6032 - kl_loss: 5.8597\n",
      "Epoch 17/30\n",
      "547/547 [==============================] - 17s 32ms/step - loss: 153.6158 - reconstruction_loss: 147.4414 - kl_loss: 5.8863\n",
      "Epoch 18/30\n",
      "547/547 [==============================] - 17s 32ms/step - loss: 152.9873 - reconstruction_loss: 147.2170 - kl_loss: 5.8976\n",
      "Epoch 19/30\n",
      "547/547 [==============================] - 18s 32ms/step - loss: 152.7796 - reconstruction_loss: 146.8125 - kl_loss: 5.9264\n",
      "Epoch 20/30\n",
      "547/547 [==============================] - 17s 32ms/step - loss: 152.1207 - reconstruction_loss: 146.6311 - kl_loss: 5.9570\n",
      "Epoch 21/30\n",
      "547/547 [==============================] - 18s 32ms/step - loss: 152.3762 - reconstruction_loss: 146.4128 - kl_loss: 5.9701\n",
      "Epoch 22/30\n",
      "547/547 [==============================] - 18s 32ms/step - loss: 152.5221 - reconstruction_loss: 146.2403 - kl_loss: 5.9881\n",
      "Epoch 23/30\n",
      "547/547 [==============================] - 17s 32ms/step - loss: 152.0186 - reconstruction_loss: 146.0787 - kl_loss: 5.9914\n",
      "Epoch 24/30\n",
      "547/547 [==============================] - 18s 32ms/step - loss: 151.6496 - reconstruction_loss: 145.8464 - kl_loss: 6.0182\n",
      "Epoch 25/30\n",
      "547/547 [==============================] - 17s 31ms/step - loss: 151.5426 - reconstruction_loss: 145.7165 - kl_loss: 6.0119\n",
      "Epoch 26/30\n",
      "547/547 [==============================] - 17s 31ms/step - loss: 151.4256 - reconstruction_loss: 145.5337 - kl_loss: 6.0235\n",
      "Epoch 27/30\n",
      "547/547 [==============================] - 16s 30ms/step - loss: 151.4967 - reconstruction_loss: 145.4191 - kl_loss: 6.0398\n",
      "Epoch 28/30\n",
      "547/547 [==============================] - 17s 31ms/step - loss: 151.2746 - reconstruction_loss: 145.3365 - kl_loss: 6.0547\n",
      "Epoch 29/30\n",
      "547/547 [==============================] - 17s 32ms/step - loss: 151.2766 - reconstruction_loss: 145.1543 - kl_loss: 6.0394\n",
      "Epoch 30/30\n",
      "547/547 [==============================] - 17s 32ms/step - loss: 151.1823 - reconstruction_loss: 144.9866 - kl_loss: 6.0721\n"
     ]
    }
   ],
   "source": [
    "mnist_digits = np.concatenate([x_train, x_test], axis=0)\n",
    "mnist_digits = np.expand_dims(mnist_digits, -1).astype(\"float32\") / 255\n",
    "\n",
    "vae = VAE(encoder, decoder)\n",
    "vae.compile(optimizer=keras.optimizers.Adam())\n",
    "history = vae.fit(mnist_digits, epochs=30, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "digit_size = 28\n",
    "scale = 1.0\n",
    "\n",
    "def generate_image(scale, digit_size):\n",
    "    x_ = random.uniform(-scale, scale) # sampling de Uniforme[-1,+1]\n",
    "    y_ = random.uniform(-scale, scale) # sampling de Uniforme[-1,+1]\n",
    "    z_sample = np.array([[x_, y_]])\n",
    "    x_decoded = vae.decoder.predict(z_sample) # pasamos por el decoder\n",
    "    digit = x_decoded[0].reshape(digit_size, digit_size) # le hacemos un reshape al formato de la imagen (28x28)\n",
    "\n",
    "    return digit\n",
    "\n",
    "def plot_image(digit):\n",
    "    plt.figure(figsize=(1,1))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(digit, cmap=\"Greys_r\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 28ms/step\n"
     ]
    }
   ],
   "source": [
    "digit = generate_image(scale, digit_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_image(digit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train[:5000,:,:]\n",
    "y_train = y_train[:5000]\n",
    "x_test = x_test[:1000,:,:]\n",
    "y_test = y_test[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 28, 28)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(5000,784)\n",
    "x_test = x_test.reshape(1000,784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.96      0.94        85\n",
      "           1       0.99      0.98      0.99       126\n",
      "           2       0.92      0.92      0.92       116\n",
      "           3       0.93      0.92      0.92       107\n",
      "           4       0.91      0.93      0.92       110\n",
      "           5       0.94      0.90      0.92        87\n",
      "           6       0.95      0.92      0.94        87\n",
      "           7       0.85      0.93      0.89        99\n",
      "           8       0.90      0.89      0.89        89\n",
      "           9       0.92      0.87      0.90        94\n",
      "\n",
      "    accuracy                           0.92      1000\n",
      "   macro avg       0.92      0.92      0.92      1000\n",
      "weighted avg       0.92      0.92      0.92      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import NuSVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "clf = make_pipeline(StandardScaler(), NuSVC(nu=0.1))\n",
    "clf.fit(x_train, y_train)\n",
    "y_pred = clf.predict(x_test)\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 17ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "digit = generate_image(scale, digit_size)\n",
    "plot_image(digit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "max = 255\n",
    "min = 0\n",
    "\n",
    "X_std = (digit - np.min(digit)) / (np.max(digit) - np.min(digit))\n",
    "X_scaled = X_std * (max - min) + min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2], dtype=uint8)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(X_scaled.reshape(1,784))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
